{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANLY-580 Week 3 Homework\n",
    "## Yi Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "* Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x).\n",
    "\n",
    "* Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations. (Only need to use different characters E.g. capitalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Student\\'s t-test\\nFrom Wikipedia, the free encyclopedia\\nThe t-test is any statistical hypothesis test in which the test statistic follows a Student\\'s t-distribution under the null hypothesis.\\nA t-test is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known. When the scaling term is unknown and is replaced by an estimate based on the data, the test statistics (under certain conditions) follow a Student\\'s t distribution. The t-test can be used, for example, to determine if two sets of data are significantly different from each other.\\n\\nWilliam Sealy Gosset, who developed the \"t-statistic\" and published it under the pseudonym of \"Student\".\\nThe t-statistic was introduced in 1908 by William Sealy Gosset, a chemist working for the Guinness brewery in Dublin, Ireland. \"Student\" was his pen name.[1][2][3][4]\\nGosset had been hired owing to Claude Guinness\\'s policy of recruiting the best graduates from Oxfo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function load(f) that reads from the file named in its sole argument, \n",
    "# and returns a string containing the text of the file.\n",
    "def load(f):\n",
    "    file = open(f)\n",
    "    return(file.read())\n",
    "\n",
    "load('corpus.txt')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"',\n",
       " '#',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '{',\n",
       " '}',\n",
       " '®',\n",
       " '–',\n",
       " '…',\n",
       " '−'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tokenizer that tokenizes the various kinds of punctuation in this text \n",
    "import nltk\n",
    "text = load('corpus.txt')\n",
    "pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "    [^\\w\\d\\s]  # Any non-alphanumeric, non-digit, non-whitespace character\n",
    "'''\n",
    "set(nltk.regexp_tokenize(text, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$12.40', '$1', '$500', '$9.99']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tokenizer that tokenizes monetary amounts\n",
    "text = 'Costs: $12.40, $1, $500, $9.99...'\n",
    "pattern = r'\\$?\\d+\\.?\\d*%?'\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22 September 2017', '09/27/2017', '09-27-2017', '09272017']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tokenizer that tokenizes dates \n",
    "text = '''This page was last edited on 22 September 2017, at 12:58.\n",
    "    Other sample dates: 09/27/2017, 09-27-2017, 09272017 ...\n",
    "'''\n",
    "pattern = r'''(?x)\n",
    "    \\d{2}[\\-|\\/]?\\d{2}[\\-|\\/]?\\d{2,4}   # match \"09/27/2017\", \"09-27-2017\", \"09272017\"\n",
    "    |\\d{1,2}\\s[A-Z]\\w+[\\.|\\,|\\s]+\\d+    # match \"22 September 2017\"\n",
    "'''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['William Sealy Gosset ',\n",
       " 'William A. Silverman: Two ',\n",
       " 'Pediatrics. ',\n",
       " 'Wikipedia® ',\n",
       " 'Wikimedia Foundation, Inc.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creater a tokenizer that tokenizes names of people and organizations\n",
    "# (find capitalized words in sequence)\n",
    "text = '''William Sealy Gosset and William A. Silverman: Two \"students\" of science\". Pediatrics. \n",
    "Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n",
    "'''\n",
    "pattern = r'''(?x) # set flag to allow verbose regexps\n",
    "  (?:[A-Z]\\w*\\S?\\s?)+ # find capitalized words in sequence\n",
    "'''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 43. With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), and NLTK's frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 0.9494017094017094\n",
      "German: 0.7065384615384616\n",
      "Chickasaw: 0.04573687182382835\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import udhr\n",
    "\n",
    "# create a function that return a list of the rank of each letter in a certain text\n",
    "def RankList(text):\n",
    "    fd = nltk.FreqDist(text.lower())\n",
    "    dist = sorted([(count, char) for char, count in dict(fd).items() if char.isalpha()], reverse = True)\n",
    "    rank = [(char, i+1) for i, (_, char) in enumerate(dist)]\n",
    "    return rank\n",
    "\n",
    "# use the following corpus to generate the rank of letters of different languages\n",
    "eng = udhr.raw('English-Latin1')\n",
    "ger = udhr.raw('German_Deutsch-Latin1')\n",
    "chi = udhr.raw('Chickasaw-Latin1')\n",
    "\n",
    "sample = load('corpus.txt')\n",
    "\n",
    "#print(RankList(eng), RankList(ger), RankList(chi))\n",
    "print('English:', nltk.spearman_correlation(RankList(eng), RankList(sample)))\n",
    "print('German:', nltk.spearman_correlation(RankList(ger), RankList(sample)))\n",
    "print('Chickasaw:', nltk.spearman_correlation(RankList(chi), RankList(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the sample's language is English since the probability is the highest with the value 94.94%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
