{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocabulary comes from pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 words in vocab\n",
      "(100000, 300) embedding matrix\n"
     ]
    }
   ],
   "source": [
    "#load our word2vec vocabulary and numpy array\n",
    "vocab_fn = \"GoogleNews_100K_vocab.txt\"\n",
    "with open(vocab_fn, 'r') as vfn:\n",
    "    index2word = vfn.read().split('\\n')\n",
    "print(len(index2word),\"words in vocab\")\n",
    "\n",
    "mat_fn = \"GoogleNews_100K.npy\"\n",
    "embedding_mat = np.load(mat_fn)\n",
    "print(embedding_mat.shape,\"embedding matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100002, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add NULL (0) and UNK (1) to our vocab\n",
    "lookup_with_unk = {word:i+2 for i,word in enumerate(index2word)}\n",
    "UNK_IND = 1\n",
    "\n",
    "#add null and UNK vectors to our embedding matrix so it still lines up\n",
    "embeddings_with_unk = np.zeros((embedding_mat.shape[0]+2, embedding_mat.shape[1]))\n",
    "embeddings_with_unk[2:] = embedding_mat\n",
    "embeddings_with_unk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Transform them into the input of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_fn = \"test.ft.txt\"\n",
    "\n",
    "sent_len = 30 # limit the sentence length of each input record\n",
    "total_records = 200000 # limit the number of input records\n",
    "count = 0\n",
    "\n",
    "# initialization\n",
    "X_matrix = np.zeros((0, sent_len), dtype=np.int32)\n",
    "labels = []\n",
    "\n",
    "with open(sent_fn, 'r') as sent_file:\n",
    "    for sent in sent_file:\n",
    "        # change sentence to lower cases; change \"don't\" to \"do not\"\n",
    "        # because \"don't\" would be tokenized to [\"do\", \"n't\"]\n",
    "        # but our vocab does not contain [\"n't\"]\n",
    "        sent = sent.lower().replace(\"n't\",\" not\")\n",
    "        # tokenize\n",
    "        tokens = word_tokenize(sent)\n",
    "        # the first token is the label, need to be forced into as 0/1\n",
    "        labels.append(int(tokens[0][9])-1) # label is 0 (negative) or 1 (positive)\n",
    "        tokens = tokens[1:]\n",
    "        # remove numbers and punctuations\n",
    "        tokens_a = [token for token in tokens if not token.isdigit() and token not in string.punctuation]\n",
    "        # look for id\n",
    "        tokens_id = [lookup_with_unk[s] if s in lookup_with_unk else UNK_IND for s in tokens_a]\n",
    "        # add 0s if the sentence is too short\n",
    "        if len(tokens_id) < sent_len:\n",
    "            tokens_id.extend([0]*(sent_len-len(tokens_id)))\n",
    "        X_matrix = np.r_[X_matrix, np.asmatrix(tokens_id[0:sent_len])] #row cat\n",
    "        count += 1\n",
    "        if count >= total_records:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  269 62311   128  8281 17245    26    47     1    13   269  6472     1\n",
      "     69  1614  4503    23  8480     1    30 62311     4    79     1  4503\n",
      "    153   748    17    63  4503   238]\n",
      " [   47     1    13   203    96   638 44046     4     1    96  4503    94\n",
      "     15   232   174   867    13   595     5  4503    23    91   409     1\n",
      "    430  2799     1    13    96    13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200000, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_matrix[0:2,])\n",
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.asarray(labels)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79287\n",
      "80713\n"
     ]
    }
   ],
   "source": [
    "# check balance\n",
    "print(len([y1 for y1 in y[:int(0.8*total_records)] if y1 == 0]))\n",
    "print(len([y1 for y1 in y[:int(0.8*total_records)] if y1 == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Train, and Test the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100002 300\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embed_size = embeddings_with_unk.shape\n",
    "print(vocab_size, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_size, \n",
    "                    name=\"word_vec\", \n",
    "                    weights=[embeddings_with_unk,]))\n",
    "\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer = \"adam\", \n",
    "              metrics=[\"accuracy\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "160000/160000 [==============================] - 199s 1ms/step - loss: 0.4200 - acc: 0.8058\n",
      "Epoch 2/3\n",
      "160000/160000 [==============================] - 193s 1ms/step - loss: 0.3028 - acc: 0.8734\n",
      "Epoch 3/3\n",
      "160000/160000 [==============================] - 193s 1ms/step - loss: 0.2698 - acc: 0.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13c749c88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the first 80% records to train the model\n",
    "model.fit(X_matrix[:int(0.8*total_records),], y[:int(0.8*total_records)], \n",
    "          epochs=3, \n",
    "          batch_size=1024, # The larger the batch, the better the approximation; \n",
    "          verbose=1) # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 12s 294us/step\n",
      "[0.28687794504165648, 0.87809999999999999]\n"
     ]
    }
   ],
   "source": [
    "# use the last 20% records to test the model\n",
    "score = model.evaluate(X_matrix[int(0.8*total_records):,], y[int(0.8*total_records):], \n",
    "                       batch_size=1024, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "160000/160000 [==============================] - 205s 1ms/step - loss: 0.2077 - acc: 0.9138\n",
      "40000/40000 [==============================] - 13s 315us/step\n",
      "[0.31486054973602295, 0.87622500000000003]\n"
     ]
    }
   ],
   "source": [
    "# use the first 80% records to train the model\n",
    "model.fit(X_matrix[:int(0.8*total_records),], y[:int(0.8*total_records)], \n",
    "          epochs=1, \n",
    "          batch_size=1024, \n",
    "          verbose=1) \n",
    "# use the last 20% records to test the model\n",
    "score = model.evaluate(X_matrix[int(0.8*total_records):,], y[int(0.8*total_records):], \n",
    "                       batch_size=1024, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del sentiment_model  # deletes the existing model\n",
    "# sentiment_model = load_model('my_model.h5') # load a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Manual Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg [ 0.00744833] This is a joke\n",
      "neg [ 0.00443599] The worst product I have ever bought\n",
      "Posi [ 0.97385174] I like it, and I want to buy it again\n",
      "neutral [ 0.52989006] I am going to give a presentation.\n",
      "neutral [ 0.71282566] I am going to have an exam.\n",
      "Posi [ 0.99185044] The new tea table looks amazing.\n",
      "Posi [ 0.99524188] This is the best class ever!\n",
      "Posi [ 0.99224257] My mom loves it.\n",
      "Posi [ 0.99034542] nice weather\n",
      "Posi [ 0.9875195] This is the best 30 bucks that I have ever spent\n",
      "neg [ 0.00449477] Complete waste of money.\n"
     ]
    }
   ],
   "source": [
    "sample_sents = [\"This is a joke\",\n",
    "                \"The worst product I have ever bought\",\n",
    "                \"I like it, and I want to buy it again\",\n",
    "                \"I am going to give a presentation.\",\n",
    "                \"I am going to have an exam.\", \n",
    "                \"The new tea table looks amazing.\", \n",
    "                \"This is the best class ever!\",\n",
    "                \"My mom loves it.\",\n",
    "                \"nice weather\",\n",
    "                \"This is the best 30 bucks that I have ever spent\",\n",
    "                \"Complete waste of money.\"\n",
    "               ]\n",
    "\n",
    "X_sample = np.zeros((0, sent_len), dtype=np.int32)\n",
    "\n",
    "for i, sent in enumerate(sample_sents):\n",
    "    sent = sent.lower().replace(\"n't\",\" not\")\n",
    "    tokens = word_tokenize(sent)\n",
    "    # remove punctuations\n",
    "    tokens_a = [token for token in tokens if token.isalpha()]\n",
    "    # look for id\n",
    "    tokens_id = [lookup_with_unk[s] if s in lookup_with_unk else UNK_IND for s in tokens_a]\n",
    "    # add 0s if the sentence is too short\n",
    "    if len(tokens_id) < sent_len:\n",
    "        tokens_id.extend([0]*(sent_len-len(tokens_id)))\n",
    "    X_sample = np.r_[X_sample, np.asmatrix(tokens_id[0:sent_len])] #row cat\n",
    "        \n",
    "results = model.predict(X_sample)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    if result > 0.8:\n",
    "        print(\"Posi\", result, sample_sents[i])\n",
    "    elif result < 0.2:\n",
    "        print(\"neg\", result, sample_sents[i])\n",
    "    else:\n",
    "        print(\"neutral\", result, sample_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i do not like 5 it! *_*\n",
      "['i', 'do', 'not', 'like', '5', 'it', '!', '*_*']\n",
      "['i', 'do', 'not', 'like', 'it', '*_*']\n",
      "[4503, 60, 15, 89, 295, 17, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "t = \"I DON'T like 5 it! *_*\"\n",
    "t = t.lower().replace(\"n't\",\" not\")\n",
    "print(t)\n",
    "t = word_tokenize(t.lower())\n",
    "print(t)\n",
    "print([t1 for t1 in t if not t1.isdigit() and t1 not in string.punctuation])\n",
    "print([lookup_with_unk[s] if s in lookup_with_unk else UNK_IND for s in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.9'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
